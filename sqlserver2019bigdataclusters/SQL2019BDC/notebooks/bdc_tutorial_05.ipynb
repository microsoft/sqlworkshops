{
    "metadata": {
        "kernelspec": {
            "name": "pyspark3kernel",
            "display_name": "PySpark3"
        },
        "language_info": {
            "name": "pyspark3",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "python",
                "version": 3
            },
            "pygments_lexer": "python3"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": "<img src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/solutions-microsoft-logo-small.png?raw=true\" alt=\"Microsoft\">\r\n<br>\r\n\r\n# SQL Server 2019 big data cluster Tutorial\r\n## 05 - Using Spark For Machine Learning\r\n\r\nIn this tutorial you will learn how to work with Spark Jobs in a SQL Server big data cluster. \r\n\r\nWide World Importers has refridgerated trucks to deliver temperature-sensitive products. These are high-profit, and high-expense items. In the past, there have been failures in the cooling systems, and the primary culprit has been the deep-cycle batteries used in the system.\r\n\r\nWWI began replacing the batters every three months as a preventative measure, but this has a high cost. Recently, the taxes on recycling batteries has increased dramatically. The CEO has asked the Data Science team if they can investigate creating a Predictive Maintenance system to more accurately tell the maintenance staff how long a battery will last, rather than relying on a flat 3 month cycle. \r\n\r\nThe trucks have sensors that transmit data to a file location. The trips are also logged. In this Jupyter Notebook, you'll create, train and store a Machine Learning model using SciKit-Learn, so that it can be deployed to multiple hosts. ",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# https://github.com/solliancenet/tech-immersion-data-ai/blob/master/day1-exp1/notebook.pdf \r\nimport pickle \r\nimport pandas as pd\r\nimport numpy as np\r\nimport datetime as dt\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn.model_selection import train_test_split",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Starting Spark application\n",
                    "output_type": "stream"
                },
                {
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>",
                        "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1561806272028_0002</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"https://40.112.221.122:30443/gateway/default/yarn/proxy/application_1561806272028_0002/\">Link</a></td><td><a target=\"_blank\" href=\"https://40.112.221.122:30443/gateway/default/yarn/container/container_1561806272028_0002_01_000001/root\">Link</a></td><td>âœ”</td></tr></table>"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "text": "SparkSession available as 'spark'.\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 3
        },
        {
            "cell_type": "code",
            "source": "",
            "metadata": {},
            "outputs": [],
            "execution_count": 0
        },
        {
            "cell_type": "markdown",
            "source": "First, download the sensor data from the location where it is transmitted from the trucks, and load it into a Spark DataFrame.",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "df = pd.read_csv('https://cs7a9736a9346a1x44c6xb00.blob.core.windows.net/backups/training-formatted.csv', header=0)\r\n\r\ndf.dropna()\r\nprint(df.shape)\r\nprint(list(df.columns))",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "text": "(10000, 74)\n['Survival_In_Days', 'Province', 'Region', 'Trip_Length_Mean', 'Trip_Length_Sigma', 'Trips_Per_Day_Mean', 'Trips_Per_Day_Sigma', 'Battery_Rated_Cycles', 'Manufacture_Month', 'Manufacture_Year', 'Alternator_Efficiency', 'Car_Has_EcoStart', 'Twelve_hourly_temperature_history_for_last_31_days_before_death_last_recording_first', 'Sensor_Reading_1', 'Sensor_Reading_2', 'Sensor_Reading_3', 'Sensor_Reading_4', 'Sensor_Reading_5', 'Sensor_Reading_6', 'Sensor_Reading_7', 'Sensor_Reading_8', 'Sensor_Reading_9', 'Sensor_Reading_10', 'Sensor_Reading_11', 'Sensor_Reading_12', 'Sensor_Reading_13', 'Sensor_Reading_14', 'Sensor_Reading_15', 'Sensor_Reading_16', 'Sensor_Reading_17', 'Sensor_Reading_18', 'Sensor_Reading_19', 'Sensor_Reading_20', 'Sensor_Reading_21', 'Sensor_Reading_22', 'Sensor_Reading_23', 'Sensor_Reading_24', 'Sensor_Reading_25', 'Sensor_Reading_26', 'Sensor_Reading_27', 'Sensor_Reading_28', 'Sensor_Reading_29', 'Sensor_Reading_30', 'Sensor_Reading_31', 'Sensor_Reading_32', 'Sensor_Reading_33', 'Sensor_Reading_34', 'Sensor_Reading_35', 'Sensor_Reading_36', 'Sensor_Reading_37', 'Sensor_Reading_38', 'Sensor_Reading_39', 'Sensor_Reading_40', 'Sensor_Reading_41', 'Sensor_Reading_42', 'Sensor_Reading_43', 'Sensor_Reading_44', 'Sensor_Reading_45', 'Sensor_Reading_46', 'Sensor_Reading_47', 'Sensor_Reading_48', 'Sensor_Reading_49', 'Sensor_Reading_50', 'Sensor_Reading_51', 'Sensor_Reading_52', 'Sensor_Reading_53', 'Sensor_Reading_54', 'Sensor_Reading_55', 'Sensor_Reading_56', 'Sensor_Reading_57', 'Sensor_Reading_58', 'Sensor_Reading_59', 'Sensor_Reading_60', 'Sensor_Reading_61']",
                    "output_type": "stream"
                }
            ],
            "execution_count": 16
        },
        {
            "cell_type": "markdown",
            "source": "After examining the data, the Data Science team selects certain columns that they believe are highly predictive of the battery life.",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Select the features used for predicting battery life\r\nx = df.iloc[:,1:74]\r\nx = x.iloc[:,np.r_[2:7, 9:73]]\r\nx = x.interpolate() \r\n\r\n# Select the labels only (the measured battery life) \r\ny = df.iloc[:,0].values.flatten()",
            "metadata": {},
            "outputs": [],
            "execution_count": 13
        },
        {
            "cell_type": "code",
            "source": "# Examine the features selected \r\nprint(list(x.columns))",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "text": "['Trip_Length_Mean', 'Trip_Length_Sigma', 'Trips_Per_Day_Mean', 'Trips_Per_Day_Sigma', 'Battery_Rated_Cycles', 'Alternator_Efficiency', 'Car_Has_EcoStart', 'Twelve_hourly_temperature_history_for_last_31_days_before_death_last_recording_first', 'Sensor_Reading_1', 'Sensor_Reading_2', 'Sensor_Reading_3', 'Sensor_Reading_4', 'Sensor_Reading_5', 'Sensor_Reading_6', 'Sensor_Reading_7', 'Sensor_Reading_8', 'Sensor_Reading_9', 'Sensor_Reading_10', 'Sensor_Reading_11', 'Sensor_Reading_12', 'Sensor_Reading_13', 'Sensor_Reading_14', 'Sensor_Reading_15', 'Sensor_Reading_16', 'Sensor_Reading_17', 'Sensor_Reading_18', 'Sensor_Reading_19', 'Sensor_Reading_20', 'Sensor_Reading_21', 'Sensor_Reading_22', 'Sensor_Reading_23', 'Sensor_Reading_24', 'Sensor_Reading_25', 'Sensor_Reading_26', 'Sensor_Reading_27', 'Sensor_Reading_28', 'Sensor_Reading_29', 'Sensor_Reading_30', 'Sensor_Reading_31', 'Sensor_Reading_32', 'Sensor_Reading_33', 'Sensor_Reading_34', 'Sensor_Reading_35', 'Sensor_Reading_36', 'Sensor_Reading_37', 'Sensor_Reading_38', 'Sensor_Reading_39', 'Sensor_Reading_40', 'Sensor_Reading_41', 'Sensor_Reading_42', 'Sensor_Reading_43', 'Sensor_Reading_44', 'Sensor_Reading_45', 'Sensor_Reading_46', 'Sensor_Reading_47', 'Sensor_Reading_48', 'Sensor_Reading_49', 'Sensor_Reading_50', 'Sensor_Reading_51', 'Sensor_Reading_52', 'Sensor_Reading_53', 'Sensor_Reading_54', 'Sensor_Reading_55', 'Sensor_Reading_56', 'Sensor_Reading_57', 'Sensor_Reading_58', 'Sensor_Reading_59', 'Sensor_Reading_60', 'Sensor_Reading_61']",
                    "output_type": "stream"
                }
            ],
            "execution_count": 14
        },
        {
            "cell_type": "markdown",
            "source": "The lead Data Scientist believes that a standard Regression algorithm would do the best predictions.",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Train a regression model \r\nfrom sklearn.ensemble import GradientBoostingRegressor \r\nmodel = GradientBoostingRegressor() \r\nmodel.fit(x,y)",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "text": "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n             max_leaf_nodes=None, min_impurity_decrease=0.0,\n             min_impurity_split=None, min_samples_leaf=1,\n             min_samples_split=2, min_weight_fraction_leaf=0.0,\n             n_estimators=100, n_iter_no_change=None, presort='auto',\n             random_state=None, subsample=1.0, tol=0.0001,\n             validation_fraction=0.1, verbose=0, warm_start=False)",
                    "output_type": "stream"
                }
            ],
            "execution_count": 15
        },
        {
            "cell_type": "code",
            "source": "# Try making a single prediction and observe the result \r\nmodel.predict(x.iloc[0:1]) ",
            "metadata": {},
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "array([1323.39791998])"
                }
            ],
            "execution_count": 8
        },
        {
            "cell_type": "markdown",
            "source": "After the model is trained, perform testing from labeled data.",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# access the test data from HDFS by reading into a Spark DataFrame \r\ntest_data = pd.read_csv('https://raw.githubusercontent.com/solliancenet/tech-immersion-data-ai/master/environment-setup/data/1/fleet-formatted.csv', header=0)\r\ntest_data.dropna()\r\n\r\n# prepare the test data (dropping unused columns) \r\ntest_data = test_data.drop(columns=[\"Car_ID\", \"Battery_Age\"])\r\ntest_data = test_data.iloc[:,np.r_[2:7, 9:73]]\r\ntest_data.rename(columns={'Twelve_hourly_temperature_forecast_for_next_31_days _reversed': 'Twelve_hourly_temperature_history_for_last_31_days_before_death_l ast_recording_first'}, inplace=True) \r\n# make the battery life predictions for each of the vehicles in the test data \r\nbattery_life_predictions = model.predict(test_data) \r\n# examine the prediction \r\nbattery_life_predictions",
            "metadata": {},
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "array([1472.91111228, 1340.08897725, 1421.38601032, 1473.79033215,\n       1651.66584142, 1412.85617044, 1842.81351408, 1264.22762055,\n       1930.45602533, 1681.86345995])"
                }
            ],
            "execution_count": 9
        },
        {
            "cell_type": "code",
            "source": "# prepare one data frame that includes predictions for each vehicle \r\nscored_data = test_data \r\nscored_data[\"Estimated_Battery_Life\"] = battery_life_predictions \r\ndf_scored = spark.createDataFrame(scored_data) \r\n# Write out the scored data: \r\ndf_scored.coalesce(1).write.option(\"header\", \"true\").csv(\"/pdm\") ",
            "metadata": {},
            "outputs": [],
            "execution_count": 10
        },
        {
            "cell_type": "markdown",
            "source": "Once you are satisfied with the Model, you can save it out using the \"Pickle\" library for deployment to other systems.",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "pickle_file = open('/tmp/pdm.pkl', 'wb')\r\npickle.dump(model, pickle_file)\r\nimport os\r\nprint(os.getcwd())\r\nos.listdir('///tmp')",
            "metadata": {},
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "/tmp/nm-local-dir/usercache/root/appcache/application_1556393889616_0002/container_1556393889616_0002_01_000001\n['hsperfdata_root', 'Jetty_localhost_45131_datanode____.y2vugb', 'nm-local-dir', 'Jetty_0_0_0_0_8042_node____19tj0x', 'tmp0c7_cdnk', 'pdm.pkl']"
                }
            ],
            "execution_count": 11
        },
        {
            "cell_type": "markdown",
            "source": "**(Optional)**\r\n\r\nYou could export this model and <a href=\"https://azure.microsoft.com/en-us/services/sql-database-edge/\" target=\"_blank\">run it at the edge or in SQL Server directly</a>. Here's an example of what that code could look like:\r\n\r\n<pre>\r\n\r\nDECLARE @query_string nvarchar(max) -- Query Truck Data\r\nSET @query_string='\r\nSELECT ['Trip_Length_Mean', 'Trip_Length_Sigma', 'Trips_Per_Day_Mean', 'Trips_Per_Day_Sigma', 'Battery_Rated_Cycles', 'Alternator_Efficiency', 'Car_Has_EcoStart', 'Twelve_hourly_temperature_history_for_last_31_days_before_death_last_recording_first', 'Sensor_Reading_1', 'Sensor_Reading_2', 'Sensor_Reading_3', 'Sensor_Reading_4', 'Sensor_Reading_5', 'Sensor_Reading_6', 'Sensor_Reading_7', 'Sensor_Reading_8', 'Sensor_Reading_9', 'Sensor_Reading_10', 'Sensor_Reading_11', 'Sensor_Reading_12', 'Sensor_Reading_13', 'Sensor_Reading_14', 'Sensor_Reading_15', 'Sensor_Reading_16', 'Sensor_Reading_17', 'Sensor_Reading_18', 'Sensor_Reading_19', 'Sensor_Reading_20', 'Sensor_Reading_21', 'Sensor_Reading_22', 'Sensor_Reading_23', 'Sensor_Reading_24', 'Sensor_Reading_25', 'Sensor_Reading_26', 'Sensor_Reading_27', 'Sensor_Reading_28', 'Sensor_Reading_29', 'Sensor_Reading_30', 'Sensor_Reading_31', 'Sensor_Reading_32', 'Sensor_Reading_33', 'Sensor_Reading_34', 'Sensor_Reading_35', 'Sensor_Reading_36', 'Sensor_Reading_37', 'Sensor_Reading_38', 'Sensor_Reading_39', 'Sensor_Reading_40', 'Sensor_Reading_41', 'Sensor_Reading_42', 'Sensor_Reading_43', 'Sensor_Reading_44', 'Sensor_Reading_45', 'Sensor_Reading_46', 'Sensor_Reading_47', 'Sensor_Reading_48', 'Sensor_Reading_49', 'Sensor_Reading_50', 'Sensor_Reading_51', 'Sensor_Reading_52', 'Sensor_Reading_53', 'Sensor_Reading_54', 'Sensor_Reading_55', 'Sensor_Reading_56', 'Sensor_Reading_57', 'Sensor_Reading_58', 'Sensor_Reading_59', 'Sensor_Reading_60', 'Sensor_Reading_61']\r\nFROM Truck_Sensor_Readings'\r\nEXEC [dbo].[PredictBattLife] 'pdm', @query_string;\r\n\r\n</pre>\r\n",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "## Next Steps: Continue on to other workloads in SQL Server 2019\r\n\r\nNow you're ready to work with SQL Server 2019's other features - [you can learn more about those here](https://docs.microsoft.com/en-us/sql/big-data-cluster/big-data-cluster-overview?view=sqlallproducts-allversions).",
            "metadata": {}
        }
    ]
}