{
    "metadata": {
        "kernelspec": {
            "name": "pyspark3kernel",
            "display_name": "PySpark3"
        },
        "language_info": {
            "name": "pyspark3",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "python",
                "version": 3
            },
            "pygments_lexer": "python3"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": "<img src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/solutions-microsoft-logo-small.png?raw=true\" alt=\"Microsoft\">\r\n<br>\r\n\r\n# SQL Server 2019 big data cluster Tutorial\r\n## 04 - Using Spark for ETL\r\n\r\nIn this tutorial you will learn how to work with Spark Jobs in a SQL Server big data cluster. \r\n\r\nMany times Spark is used to do transformations on data at large scale. In this Jupyter Notebook, you'll read a large text file into a Spark DataFrame, and then save out the top 10 examples as a table using SparkSQL.",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "# Read the product reviews CSV files into a spark data frame, print schema & top rows\r\nresults = spark.read.option(\"inferSchema\", \"true\").csv('/product_review_data').toDF(\"Item_ID\", \"Review\")",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "text": "Starting Spark application\n",
                    "output_type": "stream"
                },
                {
                    "data": {
                        "text/plain": "<IPython.core.display.HTML object>",
                        "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1561806272028_0001</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"https://40.112.221.122:30443/gateway/default/yarn/proxy/application_1561806272028_0001/\">Link</a></td><td><a target=\"_blank\" href=\"https://40.112.221.122:30443/gateway/default/yarn/container/container_1561806272028_0001_01_000001/root\">Link</a></td><td>âœ”</td></tr></table>"
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "text": "SparkSession available as 'spark'.\n",
                    "output_type": "stream"
                }
            ],
            "execution_count": 3
        },
        {
            "cell_type": "code",
            "source": "# Save results as parquet file and create hive table\r\nresults.write.format(\"parquet\").mode(\"overwrite\").saveAsTable(\"Top_Product_Reviews\")",
            "metadata": {},
            "outputs": [],
            "execution_count": 4
        },
        {
            "cell_type": "code",
            "source": "# Execute Spark SQL commands\r\nsqlDF = spark.sql(\"SELECT * FROM Top_Product_Reviews LIMIT 10\")\r\nsqlDF.show()",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "text": "+-------+--------------------+\n|Item_ID|              Review|\n+-------+--------------------+\n|  72621|Works fine. Easy ...|\n|  89334|great product to ...|\n|  89335|Next time will go...|\n|  84259|Great Gift Great ...|\n|  84398|After trip to Par...|\n|  66434|Simply the best t...|\n|  66501|This is the exact...|\n|  66587|Not super magnet;...|\n|  66680|Installed as bath...|\n|  66694|Our home was buil...|\n+-------+--------------------+",
                    "output_type": "stream"
                }
            ],
            "execution_count": 5
        },
        {
            "cell_type": "markdown",
            "source": "## Next Steps: Continue on to Working with Spark and Machine Learning\r\n\r\nNow you're ready to open the final Python Notebook in this tutorial series - `bdc_tutorial_05.ipynb` - to learn how to create and work with Spark and Machine Learning.",
            "metadata": {}
        }
    ]
}